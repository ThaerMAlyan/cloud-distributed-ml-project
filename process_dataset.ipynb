{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163269ae-d3eb-4df4-ad84-4df4fdc6cba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 800001\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rows': 800001,\n",
       " 'cols': 3,\n",
       " 'unique_cities': 3,\n",
       " 'avg_age': 20.333333333333332,\n",
       " 'stats_sec': 1.3527014255523682,\n",
       " 'prep_sec': 0.0005958080291748047,\n",
       " 'lr_sec': 2.427687406539917,\n",
       " 'kmeans_sec': 3.416335105895996,\n",
       " 'classif_sec': 8.515571355819702,\n",
       " 'total_sec': 15.71289610862732}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 266667   \n",
    "\n",
    "big_df_800k = df.crossJoin(\n",
    "    spark.range(N).select(col(\"id\").alias(\"rid\"))\n",
    ").drop(\"rid\")\n",
    "\n",
    "print(\"Rows:\", big_df_800k.count())\n",
    "\n",
    "bench_800k = run_benchmark(big_df_800k)\n",
    "bench_800k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63500d9-617f-4c23-91f7-7227c5e37e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 400002\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rows': 400002,\n",
       " 'cols': 3,\n",
       " 'unique_cities': 3,\n",
       " 'avg_age': 20.333333333333332,\n",
       " 'stats_sec': 2.0248830318450928,\n",
       " 'prep_sec': 0.0007207393646240234,\n",
       " 'lr_sec': 2.488649606704712,\n",
       " 'kmeans_sec': 3.353914976119995,\n",
       " 'classif_sec': 7.114344120025635,\n",
       " 'total_sec': 14.982517957687378}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 133334   \n",
    "\n",
    "big_df_400k = df.crossJoin(\n",
    "    spark.range(N).select(col(\"id\").alias(\"rid\"))\n",
    ").drop(\"rid\")\n",
    "\n",
    "print(\"Rows:\", big_df_400k.count())\n",
    "\n",
    "bench_400k = run_benchmark(big_df_400k)\n",
    "bench_400k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6ec519-34dc-41b7-ae41-16fa4691d45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 200001\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rows': 200001,\n",
       " 'cols': 3,\n",
       " 'unique_cities': 3,\n",
       " 'avg_age': 20.333333333333332,\n",
       " 'stats_sec': 1.405022144317627,\n",
       " 'prep_sec': 0.0005502700805664062,\n",
       " 'lr_sec': 2.5804924964904785,\n",
       " 'kmeans_sec': 3.010709285736084,\n",
       " 'classif_sec': 7.290122985839844,\n",
       " 'total_sec': 14.2869131565094}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 66667   \n",
    "\n",
    "big_df_200k = df.crossJoin(\n",
    "    spark.range(N).select(col(\"id\").alias(\"rid\"))\n",
    ").drop(\"rid\")\n",
    "\n",
    "print(\"Rows:\", big_df_200k.count())\n",
    "\n",
    "bench_200k = run_benchmark(big_df_200k)\n",
    "bench_200k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3936617e-5796-4d17-a7d0-8481dff10f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rows': 600000,\n",
       " 'cols': 3,\n",
       " 'unique_cities': 3,\n",
       " 'avg_age': 20.333333333333332,\n",
       " 'stats_sec': 1.4642035961151123,\n",
       " 'prep_sec': 0.0005917549133300781,\n",
       " 'lr_sec': 2.6159207820892334,\n",
       " 'kmeans_sec': 3.531095504760742,\n",
       " 'classif_sec': 8.11721420288086,\n",
       " 'total_sec': 15.729032039642334}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench_result = run_benchmark(big_df)\n",
    "bench_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e7e3d92-1e35-42ae-9ca5-be177d8a80f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rows': 600000,\n",
       " 'cols': 3,\n",
       " 'unique_cities': 3,\n",
       " 'avg_age': 20.333333333333332,\n",
       " 'stats_sec': 1.6856682300567627,\n",
       " 'prep_sec': 0.0005981922149658203,\n",
       " 'lr_sec': 3.949824094772339,\n",
       " 'kmeans_sec': 5.010276794433594,\n",
       " 'classif_sec': 13.1082124710083,\n",
       " 'total_sec': 23.754586219787598}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import when, array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def run_benchmark(input_df):\n",
    "    times = {}\n",
    "    total_start = time.time()\n",
    "\n",
    "    t0 = time.time()\n",
    "    rows = input_df.count()\n",
    "    cols = len(input_df.columns)\n",
    "    unique_cities = input_df.select(\"city\").distinct().count()\n",
    "    avg_age = input_df.selectExpr(\"avg(age) as avg_age\").collect()[0][\"avg_age\"]\n",
    "    times[\"stats_sec\"] = time.time() - t0\n",
    "\n",
    "    t1 = time.time()\n",
    "    assembler = VectorAssembler(inputCols=[\"age\"], outputCol=\"features\")\n",
    "    ml_df = assembler.transform(input_df).select(\"features\", \"age\", \"name\", \"city\")\n",
    "    times[\"prep_sec\"] = time.time() - t1\n",
    "\n",
    "    t2 = time.time()\n",
    "    lr_df = ml_df.select(\"features\", col(\"age\").cast(\"double\").alias(\"label\"))\n",
    "    train_lr, test_lr = lr_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    lr_model = lr.fit(train_lr)\n",
    "    _ = lr_model.transform(test_lr).count()\n",
    "    times[\"lr_sec\"] = time.time() - t2\n",
    "\n",
    "    t3 = time.time()\n",
    "    kmeans = KMeans(k=2, seed=1, featuresCol=\"features\")\n",
    "    km_model = kmeans.fit(ml_df.select(\"features\"))\n",
    "    _ = km_model.transform(ml_df.select(\"features\")).count()\n",
    "    times[\"kmeans_sec\"] = time.time() - t3\n",
    "\n",
    "    t4 = time.time()\n",
    "    cls_df = ml_df.select(\"features\", when(col(\"age\") >= 21, 1).otherwise(0).cast(\"double\").alias(\"label\"))\n",
    "    train_c, test_c = cls_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    logreg = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    cls_model = logreg.fit(train_c)\n",
    "    _ = cls_model.transform(test_c).count()\n",
    "    times[\"classif_sec\"] = time.time() - t4\n",
    "\n",
    "    times[\"total_sec\"] = time.time() - total_start\n",
    "\n",
    "    result = {\n",
    "        \"rows\": rows,\n",
    "        \"cols\": cols,\n",
    "        \"unique_cities\": unique_cities,\n",
    "        \"avg_age\": avg_age,\n",
    "        **times\n",
    "    }\n",
    "    return result\n",
    "\n",
    "bench_result = run_benchmark(big_df)\n",
    "bench_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bdd3da-7122-459a-a99e-268045ebbef4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in big_df: 600000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>city</th></tr></thead><tbody><tr><td>Ali</td><td>20.0</td><td>Gaza</td></tr><tr><td>Sara</td><td>22.0</td><td>Rafah</td></tr><tr><td>Omar</td><td>19.0</td><td>Khanyounis</td></tr><tr><td>Ali</td><td>20.0</td><td>Gaza</td></tr><tr><td>Sara</td><td>22.0</td><td>Rafah</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Ali",
         20.0,
         "Gaza"
        ],
        [
         "Sara",
         22.0,
         "Rafah"
        ],
        [
         "Omar",
         19.0,
         "Khanyounis"
        ],
        [
         "Ali",
         20.0,
         "Gaza"
        ],
        [
         "Sara",
         22.0,
         "Rafah"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "\n",
    "N = 200000\n",
    "\n",
    "big_df = df.crossJoin(spark.range(N).select(col(\"id\").alias(\"rid\"))).drop(\"rid\")\n",
    "\n",
    "print(\"Rows in big_df:\", big_df.count())\n",
    "display(big_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7298b0fb-1a24-4571-975b-45c664b60a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>items</th></tr></thead><tbody><tr><td>List(Ali, Gaza)</td></tr><tr><td>List(Sara, Rafah)</td></tr><tr><td>List(Omar, Khanyounis)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "Ali",
          "Gaza"
         ]
        ],
        [
         [
          "Sara",
          "Rafah"
         ]
        ],
        [
         [
          "Omar",
          "Khanyounis"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "items",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7214644775981576>, line 20\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
       "\u001B[1;32m     14\u001B[0m fp \u001B[38;5;241m=\u001B[39m FPGrowth(\n",
       "\u001B[1;32m     15\u001B[0m     itemsCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitems\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     16\u001B[0m     minSupport\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n",
       "\u001B[1;32m     17\u001B[0m     minConfidence\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m )\n",
       "\u001B[0;32m---> 20\u001B[0m fp_model \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mfit(fp_df)\n",
       "\u001B[1;32m     21\u001B[0m fp_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Frequent itemsets\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n",
       "\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n",
       "\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n",
       "\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    319\u001B[0m     )\n",
       "\u001B[1;32m    320\u001B[0m )\n",
       "\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n",
       "\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n",
       "\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: [CONNECT_ML.UNSUPPORTED_EXCEPTION] Generic Spark Connect ML error. FPGrowth algorithm is not supported if Spark Connect model cache offloading is enabled. SQLSTATE: XX000\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.connect.ml.MlUnsupportedException\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:248)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:562)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:615)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:591)\n",
       "\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:615)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3502)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3485)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "[CONNECT_ML.UNSUPPORTED_EXCEPTION] Generic Spark Connect ML error. FPGrowth algorithm is not supported if Spark Connect model cache offloading is enabled. SQLSTATE: XX000\n\nJVM stacktrace:\norg.apache.spark.sql.connect.ml.MlUnsupportedException\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:248)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:562)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:591)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3485)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "[CONNECT_ML.UNSUPPORTED_EXCEPTION] Generic Spark Connect ML error. FPGrowth algorithm is not supported if Spark Connect model cache offloading is enabled. SQLSTATE: XX000"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "CONNECT_ML.UNSUPPORTED_EXCEPTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XX000",
        "stackTrace": "org.apache.spark.sql.connect.ml.MlUnsupportedException\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:248)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:562)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:591)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3485)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-7214644775981576>, line 20\u001B[0m\n\u001B[1;32m     12\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     14\u001B[0m fp \u001B[38;5;241m=\u001B[39m FPGrowth(\n\u001B[1;32m     15\u001B[0m     itemsCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitems\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m     minSupport\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m,\n\u001B[1;32m     17\u001B[0m     minConfidence\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m\n\u001B[1;32m     18\u001B[0m )\n\u001B[0;32m---> 20\u001B[0m fp_model \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mfit(fp_df)\n\u001B[1;32m     21\u001B[0m fp_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Frequent itemsets\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/ml/util.py:321\u001B[0m, in \u001B[0;36mtry_remote_fit.<locals>.wrapped\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    313\u001B[0m command \u001B[38;5;241m=\u001B[39m pb2\u001B[38;5;241m.\u001B[39mCommand()\n\u001B[1;32m    314\u001B[0m command\u001B[38;5;241m.\u001B[39mml_command\u001B[38;5;241m.\u001B[39mfit\u001B[38;5;241m.\u001B[39mCopyFrom(\n\u001B[1;32m    315\u001B[0m     pb2\u001B[38;5;241m.\u001B[39mMlCommand\u001B[38;5;241m.\u001B[39mFit(\n\u001B[1;32m    316\u001B[0m         estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    319\u001B[0m     )\n\u001B[1;32m    320\u001B[0m )\n\u001B[0;32m--> 321\u001B[0m (_, properties, _) \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexecute_command(command)\n\u001B[1;32m    322\u001B[0m model_info \u001B[38;5;241m=\u001B[39m deserialize(properties)\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m warning_msg \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model_info, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarning_message\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: [CONNECT_ML.UNSUPPORTED_EXCEPTION] Generic Spark Connect ML error. FPGrowth algorithm is not supported if Spark Connect model cache offloading is enabled. SQLSTATE: XX000\n\nJVM stacktrace:\norg.apache.spark.sql.connect.ml.MlUnsupportedException\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommandOss(MLHandler.scala:248)\n\tat org.apache.spark.sql.connect.ml.MLHandler$._handleMlCommand(MLHandler.scala:562)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.$anonfun$handleMlCommand$1(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.wrapHandler(MLHandler.scala:591)\n\tat org.apache.spark.sql.connect.ml.MLHandler$.handleMlCommand(MLHandler.scala:615)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleMlCommand(SparkConnectPlanner.scala:3502)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3485)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import time\n",
    "\n",
    "fp_df = df.select(\n",
    "    array(\"name\", \"city\").alias(\"items\")\n",
    ")\n",
    "\n",
    "display(fp_df)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fp = FPGrowth(\n",
    "    itemsCol=\"items\",\n",
    "    minSupport=0.2,\n",
    "    minConfidence=0.2\n",
    ")\n",
    "\n",
    "fp_model = fp.fit(fp_df)\n",
    "fp_time = time.time() - start\n",
    "\n",
    "itemsets = fp_model.freqItemsets\n",
    "display(itemsets)\n",
    "\n",
    "rules = fp_model.associationRules\n",
    "display(rules)\n",
    "\n",
    "print(f\"FPGrowth Time: {fp_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75358dcb-6ccd-45dd-8e23-d871b9584c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>age</th><th>label</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}</td><td>20.0</td><td>0.0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}</td><td>22.0</td><td>1.0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}</td><td>19.0</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}",
         20.0,
         0.0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}",
         22.0,
         1.0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}",
         19.0,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1, \"attrs\": {\"numeric\": [{\"name\": \"age\", \"idx\": 0}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>label</th><th>rawPrediction</th><th>probability</th><th>prediction</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}</td><td>1.0</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"Infinity\",\"-Infinity\"]}</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1.0\",\"0.0\"]}</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}",
         1.0,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"Infinity\",\"-Infinity\"]}",
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1.0\",\"0.0\"]}",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1, \"attrs\": {\"numeric\": [{\"name\": \"age\", \"idx\": 0}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1}}",
         "name": "rawPrediction",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1}}",
         "name": "probability",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{\"ml_attr\": {\"num_vals\": 1, \"type\": \"nominal\"}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Time: 1.3216 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import time\n",
    "\n",
    "cls_df = ml_df.withColumn(\n",
    "    \"label\",\n",
    "    when(ml_df.age >= 21, 1).otherwise(0).cast(\"double\")\n",
    ")\n",
    "\n",
    "display(cls_df)\n",
    "\n",
    "train, test = cls_df.select(\"features\", \"label\").randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lr_cls = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "cls_model = lr_cls.fit(train)\n",
    "cls_time = time.time() - start\n",
    "\n",
    "predictions = cls_model.transform(test)\n",
    "\n",
    "display(predictions)\n",
    "\n",
    "print(f\"Classification Time: {cls_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bafd139-9ec9-460d-a8dc-5d314daf6b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>age</th><th>prediction</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}</td><td>20.0</td><td>0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}</td><td>22.0</td><td>1</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}</td><td>19.0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}",
         20.0,
         0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}",
         22.0,
         1
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}",
         19.0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1, \"attrs\": {\"numeric\": [{\"name\": \"age\", \"idx\": 0}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\": {\"num_vals\": 2, \"type\": \"nominal\"}}",
         "name": "prediction",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans Time: 3.8737 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "kmeans = KMeans(\n",
    "    k=2,\n",
    "    seed=1,\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "kmeans_model = kmeans.fit(ml_df)\n",
    "\n",
    "kmeans_time = time.time() - start\n",
    "\n",
    "clusters = kmeans_model.transform(ml_df)\n",
    "\n",
    "display(clusters)\n",
    "\n",
    "print(f\"KMeans Time: {kmeans_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f247ef75-0aa4-4f27-8f68-b9e21c7444a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Time: 2.1464 seconds\nRMSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import time\n",
    "\n",
    "lr_df = ml_df.withColumnRenamed(\"age\", \"label\")\n",
    "\n",
    "train, test = lr_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train)\n",
    "\n",
    "lr_time = time.time() - start\n",
    "\n",
    "predictions = lr_model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Linear Regression Time: {lr_time:.4f} seconds\")\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289348a7-cead-4841-b2b4-61dcca15d5cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>features</th><th>age</th></tr></thead><tbody><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}</td><td>20.0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}</td><td>22.0</td></tr><tr><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}</td><td>19.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"20.0\"]}",
         20.0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"22.0\"]}",
         22.0
        ],
        [
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"19.0\"]}",
         19.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"ml_attr\": {\"num_attrs\": 1, \"attrs\": {\"numeric\": [{\"name\": \"age\", \"idx\": 0}]}}}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_df = assembler.transform(df).select(\"features\", \"age\")\n",
    "display(ml_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5d7ed1-0909-4a6a-97df-b5d492c65c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- age: double (nullable = true)\n |-- city: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>age</th><th>city</th></tr></thead><tbody><tr><td>Ali</td><td>20.0</td><td>Gaza</td></tr><tr><td>Sara</td><td>22.0</td><td>Rafah</td></tr><tr><td>Omar</td><td>19.0</td><td>Khanyounis</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Ali",
         20.0,
         "Gaza"
        ],
        [
         "Sara",
         22.0,
         "Rafah"
        ],
        [
         "Omar",
         19.0,
         "Khanyounis"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "display(df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b33f54-fff7-47b3-b9bf-abdbe0529d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\\n    \"algorithm\": \"TimeBasedAggregation\",\\n    \"metrics\": {\\n        \"average_age\": 20.333333333333332,\\n        \"min_age\": 19.0,\\n        \"max_age\": 22.0\\n    },\\n    \"execution_time_seconds\": 1.3182177543640137\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.head(\n",
    "    \"/Volumes/workspace/default/datasets/results/aggregation_results.json\",\n",
    "    1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bf355f-96bb-4654-a7ac-8dd9cddf4d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 208 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'algorithm': 'TimeBasedAggregation',\n",
       " 'metrics': {'average_age': 20.333333333333332,\n",
       "  'min_age': 19.0,\n",
       "  'max_age': 22.0},\n",
       " 'execution_time_seconds': 5.431603670120239}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
    "\n",
    "aggregations = df.agg(\n",
    "    F.avg(\"age\").alias(\"average_age\"),\n",
    "    F.min(\"age\").alias(\"min_age\"),\n",
    "    F.max(\"age\").alias(\"max_age\")\n",
    ").collect()[0]\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "result_agg = {\n",
    "    \"algorithm\": \"TimeBasedAggregation\",\n",
    "    \"metrics\": {\n",
    "        \"average_age\": aggregations[\"average_age\"],\n",
    "        \"min_age\": aggregations[\"min_age\"],\n",
    "        \"max_age\": aggregations[\"max_age\"]\n",
    "    },\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "#  \n",
    "out_dir = \"/Volumes/workspace/default/datasets/results\"\n",
    "dbutils.fs.put(\n",
    "    f\"{out_dir}/aggregation_results.json\",\n",
    "    json.dumps(result_agg, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a0767a-f199-4061-aab9-3c9f28f3e92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 466 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'algorithm': 'DecisionTreeRegression',\n",
       " 'results': [{'name': 'Ali', 'predicted_age': 20.0, 'label': 20.0},\n",
       "  {'name': 'Sara', 'predicted_age': 22.0, 'label': 22.0},\n",
       "  {'name': 'Omar', 'predicted_age': 19.0, 'label': 19.0}],\n",
       " 'max_depth': 3,\n",
       " 'execution_time_seconds': 5.536059856414795}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df).withColumnRenamed(\"age\", \"label\")\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxDepth=3\n",
    ")\n",
    "\n",
    "model = dt.fit(data)\n",
    "\n",
    "predictions = model.transform(data)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "results = predictions.select(\n",
    "    \"name\",\n",
    "    F.col(\"prediction\").alias(\"predicted_age\"),\n",
    "    \"label\"\n",
    ").toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "result_dt = {\n",
    "    \"algorithm\": \"DecisionTreeRegression\",\n",
    "    \"results\": results,\n",
    "    \"max_depth\": 3,\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "out_dir = \"/Volumes/workspace/default/datasets/results\"\n",
    "dbutils.fs.put(\n",
    "    f\"{out_dir}/decision_tree_results.json\",\n",
    "    json.dumps(result_dt, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e2ee16-9594-4851-a637-9f63ad7f659e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 579 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'algorithm': 'LinearRegression',\n",
       " 'results': [{'name': 'Ali',\n",
       "   'predicted_age': 19.999999999999982,\n",
       "   'label': 20.0},\n",
       "  {'name': 'Sara', 'predicted_age': 22.000000000000096, 'label': 22.0},\n",
       "  {'name': 'Omar', 'predicted_age': 18.999999999999922, 'label': 19.0}],\n",
       " 'coefficients': [1.0000000000000582],\n",
       " 'intercept': -1.1821308240946304e-12,\n",
       " 'execution_time_seconds': 2.10819935798645}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df).withColumnRenamed(\"age\", \"label\")\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(data)\n",
    "\n",
    "predictions = model.transform(data)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "results = predictions.select(\n",
    "    \"name\",\n",
    "    F.col(\"prediction\").alias(\"predicted_age\"),\n",
    "    \"label\"\n",
    ").toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "result_lr = {\n",
    "    \"algorithm\": \"LinearRegression\",\n",
    "    \"results\": results,\n",
    "    \"coefficients\": model.coefficients.tolist(),\n",
    "    \"intercept\": model.intercept,\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "out_dir = \"/Volumes/workspace/default/datasets/results\"\n",
    "dbutils.fs.put(\n",
    "    f\"{out_dir}/linear_regression_results.json\",\n",
    "    json.dumps(result_lr, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706c4e40-b0d3-4853-b63e-0b8ea1146874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 419 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'algorithm': 'KMeans',\n",
       " 'k': 2,\n",
       " 'clusters': [{'name': 'Ali', 'age': 20.0, 'prediction': 1},\n",
       "  {'name': 'Sara', 'age': 22.0, 'prediction': 0},\n",
       "  {'name': 'Omar', 'age': 19.0, 'prediction': 1}],\n",
       " 'execution_time_seconds': 6.665626764297485}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "df = df.withColumn(\"age\", F.col(\"age\").cast(\"double\"))\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"age\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "features_df = assembler.transform(df)\n",
    "\n",
    "kmeans = KMeans(\n",
    "    k=2,\n",
    "    seed=42,\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "model = kmeans.fit(features_df)\n",
    "\n",
    "predictions = model.transform(features_df)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "clusters = predictions.select(\"name\", \"age\", \"prediction\").toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "result_kmeans = {\n",
    "    \"algorithm\": \"KMeans\",\n",
    "    \"k\": 2,\n",
    "    \"clusters\": clusters,\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "out_dir = \"/Volumes/workspace/default/datasets/results\"\n",
    "dbutils.fs.put(\n",
    "    f\"{out_dir}/kmeans_results.json\",\n",
    "    json.dumps(result_kmeans, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result_kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8834a609-deb1-4d5c-9bce-a9cf749c55a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 535 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'dataset_path': '/Volumes/workspace/default/datasets/sample.csv',\n",
       " 'statistics': {'rows': 3,\n",
       "  'columns': 3,\n",
       "  'data_types': {'name': 'string', 'age': 'string', 'city': 'string'},\n",
       "  'null_percentages': {'name': 0.0, 'age': 0.0, 'city': 0.0},\n",
       "  'unique_value_counts': {'name': 3, 'age': 3, 'city': 3}},\n",
       " 'execution_time_seconds': 4.42710542678833}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "path = \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    "df = spark.read.option(\"header\", \"true\").csv(path)\n",
    "\n",
    "rows = df.count()\n",
    "\n",
    "cols = len(df.columns)\n",
    "\n",
    "dtypes = {c: t for c, t in df.dtypes}\n",
    "\n",
    "null_percent = {}\n",
    "for c in df.columns:\n",
    "    nulls = df.filter(F.col(c).isNull() | (F.col(c) == \"\")).count()\n",
    "    null_percent[c] = (nulls / rows) * 100 if rows else 0\n",
    "\n",
    "unique_counts = {c: df.select(c).distinct().count() for c in df.columns}\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "result_stats = {\n",
    "    \"dataset_path\": path,\n",
    "    \"statistics\": {\n",
    "        \"rows\": rows,\n",
    "        \"columns\": cols,\n",
    "        \"data_types\": dtypes,\n",
    "        \"null_percentages\": null_percent,\n",
    "        \"unique_value_counts\": unique_counts\n",
    "    },\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "out_dir = \"/Volumes/workspace/default/datasets/results\"\n",
    "dbutils.fs.mkdirs(out_dir)\n",
    "dbutils.fs.put(\n",
    "    f\"{out_dir}/descriptive_stats.json\",\n",
    "    json.dumps(result_stats, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53ea00b-8e1e-4bbb-87da-ff9a34ced2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 214 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'statistics': {'rows': 3,\n",
       "  'columns': 3,\n",
       "  'column_names': ['name', 'age', 'city']},\n",
       " 'execution_time_seconds': 1.3308570384979248}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\n",
    "    \"/Volumes/workspace/default/datasets/sample.csv\"\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    \"rows\": df.count(),\n",
    "    \"columns\": len(df.columns),\n",
    "    \"column_names\": df.columns\n",
    "}\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "result = {\n",
    "    \"statistics\": stats,\n",
    "    \"execution_time_seconds\": execution_time\n",
    "}\n",
    "\n",
    "dbutils.fs.mkdirs(\"/Volumes/workspace/default/datasets/results\")\n",
    "\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/workspace/default/datasets/results/output.json\",\n",
    "    json.dumps(result, indent=4),\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "process_dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}